{"cells":[{"cell_type":"markdown","source":["## Functions covered here:\n* collect_list\n* array_contains\n* arrays_zip\n* map_from_arrays\n* array_distinct\n* array_except\n* array_intersect\n* array_join\n* array_remove\n* element_at\n* array_position\n* array_max\n* array_repeat\n* array_sort\n* sort_array\n* array_union\n* concat\n* array\n* flatten\n* arrays_overlap\n* reverse\n* shuffle\n* size\n* slice\n* explode\n* posexplode\n* posexplode_outer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f316fa9a-143d-436f-9fb7-12cb337a8d7b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark.sql.functions as f\ninitial_df = sc.parallelize([(\"x\", 4, 1),\n  (\"x\", 6, 2),\n  (\"z\", 7, 3),\n  (\"a\", 3, 4),\n  (\"z\", 5, 2),\n  (\"x\", 7, 3),\n  (\"x\", 9, 7),\n  (\"z\", 1, 8),\n  (\"z\", 4, 9),\n  (\"z\", 7, 4),\n  (\"a\", 8, 5),\n  (\"a\", 5, 2),\n  (\"a\", 3, 8),\n  (\"x\", 2, 7),\n  (\"z\", 1, 9)]).toDF([\"col1\", \"col2\", \"col3\"])\ninitial_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e97f20bc-2d82-455a-b165-217b4235fc1b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   x|   4|   1|\n|   x|   6|   2|\n|   z|   7|   3|\n|   a|   3|   4|\n|   z|   5|   2|\n|   x|   7|   3|\n|   x|   9|   7|\n|   z|   1|   8|\n|   z|   4|   9|\n|   z|   7|   4|\n|   a|   8|   5|\n|   a|   5|   2|\n|   a|   3|   8|\n|   x|   2|   7|\n|   z|   1|   9|\n+----+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["initial_df.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"428c951e-ecdd-44d3-b47d-c2aab3773d6d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[2]: [('col1', 'string'), ('col2', 'bigint'), ('col3', 'bigint')]"]}],"execution_count":0},{"cell_type":"code","source":["df=initial_df.groupBy(\"col1\").agg(f.collect_list(\"col2\").alias(\"array_c2\"),f.collect_list(\"col3\").alias(\"array_c3\"))\ndf.show()\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"888f1aaa-843f-42f0-b27c-78f0c615f0b7","inputWidgets":{},"title":"collect_list"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+\n|col1|          array_c2|          array_c3|\n+----+------------------+------------------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|\n+----+------------------+------------------+\n\nroot\n |-- col1: string (nullable = true)\n |-- array_c2: array (nullable = false)\n |    |-- element: long (containsNull = false)\n |-- array_c3: array (nullable = false)\n |    |-- element: long (containsNull = false)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select('col1',f.explode('array_c2')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e347d2b-49f4-483a-b71e-4a438db956c5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+---+\n|col1|col|\n+----+---+\n|   x|  4|\n|   x|  6|\n|   x|  7|\n|   x|  9|\n|   x|  2|\n|   z|  7|\n|   z|  5|\n|   z|  1|\n|   z|  4|\n|   z|  7|\n|   z|  1|\n|   a|  3|\n|   a|  8|\n|   a|  5|\n|   a|  3|\n+----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_contains(\"array_c2\",3)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"fdaed711-0ada-4980-9c9d-5d5bd06dda6c","inputWidgets":{},"title":"array_contains"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]| false|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]| false|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  true|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.arrays_zip(\"array_c2\",\"array_c3\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8e1a90a3-a732-431a-92dc-43dc95523771","inputWidgets":{},"title":"arrays_zip"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------------------------------------+\n|col1|array_c2          |array_c3          |result                                          |\n+----+------------------+------------------+------------------------------------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[{4, 1}, {6, 2}, {7, 3}, {9, 7}, {2, 7}]        |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[{7, 3}, {5, 2}, {1, 8}, {4, 9}, {7, 4}, {1, 9}]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[{3, 4}, {8, 5}, {5, 2}, {3, 8}]                |\n+----+------------------+------------------+------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df3=spark.createDataFrame([([2, 5,2], ['a', 'b','c'])], schema=['k', 'v'])\ndf3.show()\ndf3.withColumn(\"result\",f.map_from_arrays(\"k\",\"v\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b3b3cc38-e64e-41ad-975f-ae57a18453e5","inputWidgets":{},"title":"map_from_arrays -- error if duplicate keys are available"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+\n|        k|        v|\n+---------+---------+\n|[2, 5, 2]|[a, b, c]|\n+---------+---------+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2456600327339300>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'b'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'c'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'k'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'v'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"result\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_from_arrays\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"k\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"v\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    508\u001B[0m                     \"Parameter 'truncate={}' should be either bool or int.\".format(truncate))\n\u001B[1;32m    509\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 510\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint_truncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    511\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    512\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o755.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 57.0 failed 1 times, most recent failure: Lost task 2.0 in stage 57.0 (TID 162) (ip-10-172-223-131.us-west-2.compute.internal executor driver): java.lang.RuntimeException: Duplicate map key 2 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.duplicateMapKeyFoundError(QueryExecutionErrors.scala:1018)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:69)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.putAll(ArrayBasedMapBuilder.scala:94)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.from(ArrayBasedMapBuilder.scala:122)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2636)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:587)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:596)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:542)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:541)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:438)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:417)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:422)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3132)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3938)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2856)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:360)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:310)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2856)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3063)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:293)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:332)\n\tat sun.reflect.GeneratedMethodAccessor516.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: Duplicate map key 2 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.duplicateMapKeyFoundError(QueryExecutionErrors.scala:1018)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:69)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.putAll(ArrayBasedMapBuilder.scala:94)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.from(ArrayBasedMapBuilder.scala:122)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 57.0 failed 1 times, most recent failure: Lost task 2.0 in stage 57.0 (TID 162) (ip-10-172-223-131.us-west-2.compute.internal executor driver): java.lang.RuntimeException: Duplicate map key 2 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n","\u001B[0;32m<command-2456600327339300>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'b'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'c'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'k'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'v'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      2\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"result\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_from_arrays\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"k\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"v\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n","\u001B[1;32m    508\u001B[0m                     \"Parameter 'truncate={}' should be either bool or int.\".format(truncate))\n","\u001B[1;32m    509\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 510\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint_truncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    511\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    512\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n","\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n","\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n","\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o755.showString.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 57.0 failed 1 times, most recent failure: Lost task 2.0 in stage 57.0 (TID 162) (ip-10-172-223-131.us-west-2.compute.internal executor driver): java.lang.RuntimeException: Duplicate map key 2 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.duplicateMapKeyFoundError(QueryExecutionErrors.scala:1018)\n","\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:69)\n","\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.putAll(ArrayBasedMapBuilder.scala:94)\n","\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.from(ArrayBasedMapBuilder.scala:122)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n","\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n","\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2636)\n","\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n","\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n","\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n","\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n","\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n","\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:587)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:596)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:542)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:541)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:438)\n","\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:417)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:422)\n","\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3132)\n","\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3938)\n","\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2856)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:213)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:360)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:310)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n","\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2856)\n","\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3063)\n","\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:293)\n","\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:332)\n","\tat sun.reflect.GeneratedMethodAccessor516.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:295)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: java.lang.RuntimeException: Duplicate map key 2 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.duplicateMapKeyFoundError(QueryExecutionErrors.scala:1018)\n","\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:69)\n","\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.putAll(ArrayBasedMapBuilder.scala:94)\n","\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.from(ArrayBasedMapBuilder.scala:122)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n","\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n","\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n","\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n","\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["df3=spark.createDataFrame([([2, 5,7], ['a', 'b','c'])], schema=['k', 'v'])\ndf3.show()\ndf3.withColumn(\"result\",f.map_from_arrays(\"k\",\"v\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"ad44bfc5-5441-4d3f-8153-fca8a294b5c1","inputWidgets":{},"title":"map_from_arrays"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+\n|        k|        v|\n+---------+---------+\n|[2, 5, 7]|[a, b, c]|\n+---------+---------+\n\n+---------+---------+------------------------+\n|k        |v        |result                  |\n+---------+---------+------------------------+\n|[2, 5, 7]|[a, b, c]|{2 -> a, 5 -> b, 7 -> c}|\n+---------+---------+------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_distinct(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"dc8b6d7f-105d-43be-89e3-f9d01ef2cfe0","inputWidgets":{},"title":"array_distinct"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+---------------+\n|col1|          array_c2|          array_c3|         result|\n+----+------------------+------------------+---------------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|[4, 6, 7, 9, 2]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|   [7, 5, 1, 4]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|      [3, 8, 5]|\n+----+------------------+------------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_except(\"array_c3\",\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8f18bd96-37ae-4e1f-bf3d-fc3675ff590c","inputWidgets":{},"title":"array_except - subtract operation"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------+\n|col1|          array_c2|          array_c3|      result|\n+----+------------------+------------------+------------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|      [1, 3]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[3, 2, 8, 9]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|      [4, 2]|\n+----+------------------+------------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_intersect(\"array_c2\",\"array_c3\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"5955da94-1d90-457e-addd-a6347815d9ec","inputWidgets":{},"title":"array_intersect"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|[7, 2]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|   [4]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|[8, 5]|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_join(\"array_c2\",\"-\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"20d3b350-d6a7-47ba-87cb-0d11009a573f","inputWidgets":{},"title":"array_join"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+-----------+\n|col1|          array_c2|          array_c3|     result|\n+----+------------------+------------------+-----------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  4-6-7-9-2|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|7-5-1-4-7-1|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|    3-8-5-3|\n+----+------------------+------------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_max(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"5bc7710a-1205-4b15-8909-0ee61b225928","inputWidgets":{},"title":"array_max"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     9|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     7|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     8|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_position(\"array_c2\",7)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"51933e67-9aca-4969-8caf-88c118aad89b","inputWidgets":{},"title":"array_position - position of given element"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     3|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     1|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     0|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.element_at(\"array_c3\",3)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"f3f59847-fcfd-4290-93b6-cdb75f9c6e66","inputWidgets":{},"title":"element_at -- element at given position"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     3|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     8|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     2|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_remove(\"array_c2\",7)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b450f9ff-497b-4caf-97cf-57d3b1bb02dc","inputWidgets":{},"title":"array_remove"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------+\n|col1|          array_c2|          array_c3|      result|\n+----+------------------+------------------+------------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|[4, 6, 9, 2]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[5, 1, 4, 1]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|[3, 8, 5, 3]|\n+----+------------------+------------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_repeat(\"array_c2\",5)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"2d757019-b9d7-4175-a991-9c6223314740","inputWidgets":{},"title":"array_repeat"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+----------------------------------------------------------------------------------------------------+\n|col1|array_c2          |array_c3          |result                                                                                              |\n+----+------------------+------------------+----------------------------------------------------------------------------------------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[[4, 6, 7, 9, 2], [4, 6, 7, 9, 2], [4, 6, 7, 9, 2], [4, 6, 7, 9, 2], [4, 6, 7, 9, 2]]               |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[[7, 5, 1, 4, 7, 1], [7, 5, 1, 4, 7, 1], [7, 5, 1, 4, 7, 1], [7, 5, 1, 4, 7, 1], [7, 5, 1, 4, 7, 1]]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[[3, 8, 5, 3], [3, 8, 5, 3], [3, 8, 5, 3], [3, 8, 5, 3], [3, 8, 5, 3]]                              |\n+----+------------------+------------------+----------------------------------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_sort(\"array_c2\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9cf47358-2aa0-47e0-bf90-d722f66ab1dd","inputWidgets":{},"title":"array_sort"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------+\n|col1|array_c2          |array_c3          |result            |\n+----+------------------+------------------+------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[2, 4, 6, 7, 9]   |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[1, 1, 4, 5, 7, 7]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[3, 3, 5, 8]      |\n+----+------------------+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.sort_array(\"array_c2\",asc=False)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"da931f51-c0a6-4a4a-91f3-7e823a887a06","inputWidgets":{},"title":"sort_array"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------+\n|col1|array_c2          |array_c3          |result            |\n+----+------------------+------------------+------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[9, 7, 6, 4, 2]   |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[7, 7, 5, 4, 1, 1]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[8, 5, 3, 3]      |\n+----+------------------+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array_union(\"array_c2\",\"array_c3\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"0dfc7016-8003-4e0c-ad33-7f04452113a2","inputWidgets":{},"title":"array_union - removes duplicates"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------------+\n|col1|array_c2          |array_c3          |result                  |\n+----+------------------+------------------+------------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[4, 6, 7, 9, 2, 1, 3]   |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[7, 5, 1, 4, 3, 2, 8, 9]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[3, 8, 5, 4, 2]         |\n+----+------------------+------------------+------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.concat(\"array_c2\",\"array_c3\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"010e91af-eaef-4c67-8c75-aeede608025f","inputWidgets":{},"title":"concat"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------------------------+\n|col1|array_c2          |array_c3          |result                              |\n+----+------------------+------------------+------------------------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[4, 6, 7, 9, 2, 1, 2, 3, 7, 7]      |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[7, 5, 1, 4, 7, 1, 3, 2, 8, 9, 4, 9]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[3, 8, 5, 3, 4, 5, 2, 8]            |\n+----+------------------+------------------+------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.array(\"array_c2\",\"array_c3\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"4937db31-8486-48be-b60c-66320d11c3cf","inputWidgets":{},"title":"array"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+----------------------------------------+\n|col1|array_c2          |array_c3          |result                                  |\n+----+------------------+------------------+----------------------------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[[4, 6, 7, 9, 2], [1, 2, 3, 7, 7]]      |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[[7, 5, 1, 4, 7, 1], [3, 2, 8, 9, 4, 9]]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[[3, 8, 5, 3], [4, 5, 2, 8]]            |\n+----+------------------+------------------+----------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.withColumn(\"result\",f.array(\"array_c2\",\"array_c3\"))\ndf2.withColumn(\"result1\",f.flatten(\"result\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"3ec44202-2dfe-4b26-af57-35f5f8164413","inputWidgets":{},"title":"flatten"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+----------------------------------------+------------------------------------+\n|col1|array_c2          |array_c3          |result                                  |result1                             |\n+----+------------------+------------------+----------------------------------------+------------------------------------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |[[4, 6, 7, 9, 2], [1, 2, 3, 7, 7]]      |[4, 6, 7, 9, 2, 1, 2, 3, 7, 7]      |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[[7, 5, 1, 4, 7, 1], [3, 2, 8, 9, 4, 9]]|[7, 5, 1, 4, 7, 1, 3, 2, 8, 9, 4, 9]|\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |[[3, 8, 5, 3], [4, 5, 2, 8]]            |[3, 8, 5, 3, 4, 5, 2, 8]            |\n+----+------------------+------------------+----------------------------------------+------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.arrays_overlap(\"array_c2\",\"array_c3\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9edf8c46-cfdc-4338-a1e6-1f2f4720f895","inputWidgets":{},"title":"arrays_overlap"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|array_c2          |array_c3          |result|\n+----+------------------+------------------+------+\n|x   |[4, 6, 7, 9, 2]   |[1, 2, 3, 7, 7]   |true  |\n|z   |[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|true  |\n|a   |[3, 8, 5, 3]      |[4, 5, 2, 8]      |true  |\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.reverse(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9b2e647f-372c-45d6-b7f3-505f1904cb25","inputWidgets":{},"title":"reverse"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------+\n|col1|          array_c2|          array_c3|            result|\n+----+------------------+------------------+------------------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|   [2, 9, 7, 6, 4]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[1, 7, 4, 1, 5, 7]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|      [3, 5, 8, 3]|\n+----+------------------+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.shuffle(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"003582fe-a39e-478f-8cd4-720732b1fb33","inputWidgets":{},"title":"shuffle"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------------------+\n|col1|          array_c2|          array_c3|            result|\n+----+------------------+------------------+------------------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|   [9, 4, 2, 6, 7]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[1, 5, 4, 7, 7, 1]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|      [8, 5, 3, 3]|\n+----+------------------+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.size(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"e1549731-d78f-4c4c-bcae-7b2e71989b0e","inputWidgets":{},"title":"size"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     5|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     6|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     4|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.slice(\"array_c2\",2,3)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"c835f9de-bb3e-47cc-91bb-d184005e264e","inputWidgets":{},"title":"slice"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+---------+\n|col1|          array_c2|          array_c3|   result|\n+----+------------------+------------------+---------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|[6, 7, 9]|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|[5, 1, 4]|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|[8, 5, 3]|\n+----+------------------+------------------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"result\",f.explode(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9d14c302-e7ae-45cb-9d32-b6475535e15f","inputWidgets":{},"title":"explode"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+------+\n|col1|          array_c2|          array_c3|result|\n+----+------------------+------------------+------+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     4|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     6|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     7|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     9|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|     2|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     7|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     5|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     1|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     4|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     7|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|     1|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     3|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     8|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     5|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|     3|\n+----+------------------+------------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(\"*\",f.posexplode(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"e4ceb37b-25cd-47de-9f6f-6a516441ad10","inputWidgets":{},"title":"posexplode"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+---+---+\n|col1|          array_c2|          array_c3|pos|col|\n+----+------------------+------------------+---+---+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  0|  4|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  1|  6|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  2|  7|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  3|  9|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  4|  2|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  0|  7|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  1|  5|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  2|  1|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  3|  4|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  4|  7|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  5|  1|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  0|  3|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  1|  8|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  2|  5|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  3|  3|\n+----+------------------+------------------+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\ndf.select(\"*\",f.posexplode_outer(\"array_c2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"f2cced48-3c5d-41ae-bce1-347815fd21d5","inputWidgets":{},"title":"posexplode_outer"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------------------+------------------+---+---+\n|col1|          array_c2|          array_c3|pos|col|\n+----+------------------+------------------+---+---+\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  0|  4|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  1|  6|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  2|  7|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  3|  9|\n|   x|   [4, 6, 7, 9, 2]|   [1, 2, 3, 7, 7]|  4|  2|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  0|  7|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  1|  5|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  2|  1|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  3|  4|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  4|  7|\n|   z|[7, 5, 1, 4, 7, 1]|[3, 2, 8, 9, 4, 9]|  5|  1|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  0|  3|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  1|  8|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  2|  5|\n|   a|      [3, 8, 5, 3]|      [4, 5, 2, 8]|  3|  3|\n+----+------------------+------------------+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2c28f85-2da9-43ea-a207-6f2a977be21b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark Arrays","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
